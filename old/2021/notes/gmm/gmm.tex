\documentclass[11pt,letterpaper]{article}

\usepackage[top=1in, 
left=1in, 
right=1in, 
bottom=1in]{geometry}
\usepackage{setspace}
\usepackage{titling}
\newcommand{\subtitle}[1]{%
	\posttitle{%
		\par\end{center}
	\begin{center}\large#1\end{center}
	\vskip0.5em}%
}

\usepackage{lmodern}
\usepackage{amssymb,amsmath,bm,bbm}
\renewcommand{\familydefault}{\sfdefault}

\DeclareMathOperator*{\argmin}{argmin}
\DeclareMathOperator*{\plim}{plim}

\usepackage{booktabs,caption,subcaption,threeparttable}
\usepackage{graphicx}

\usepackage[hyperfootnotes=false, 
colorlinks=true, 
allcolors=black]{hyperref}

\usepackage[backend=biber, 
authordate, 
maxcitenames=2, 
uniquename=false, 
uniquelist=false, 
url=false, 
doi=false, 
isbn=false]{biblatex-chicago}
\addbibresource{refs.bib}
\usepackage{bibentry}
\setlength{\bibhang}{0pt}

\begin{document}

\title{Generalized Method of Moments}
\subtitle{Topics in Advanced Econometrics (ResEcon 703)\vspace{-2ex}}
\author{Matt Woerman\\Resource Economics, UMass Amherst}
%\date{}  % Toggle commenting to test
\date{\vspace{-5ex}}

\maketitle

\section{Overview of method of moments}

Generalized method of moments (GMM) and its special case, method of moments (MM), are two common estimation methods in modern empirical economics. These methods are often used to estimate structural econometric models and also have frequent applications in the fields of macroeconomics and finance. A primary reason that these methods are commonly used is that they are semi-parametric, making them more flexible than fully parametric methods, such as maximum likelihood estimation. In particular, MM and GMM require assumptions about only moments of the data, rather than the full distribution of the data. Additionally, many other estimation methods---ordinary least squares, two-stage least squares, generalized least squares, maximum likelihood estimation, etc.---generate moment conditions that can be used with MM and GMM, meaning that GMM is a highly general estimation method that encompasses most other estimators typically found in applied microeconomics.

\subsection{Moment conditions}

The central assumption of (generalized) method of moments estimation is that we know moment conditions about the population from which our data are drawn. These moment conditions are functions of parameters and data that equal zero in expectation when evaluated at the true parameter values. Simple examples of moments conditions are generated by the mean and variance of an i.i.d.\ random variable, as well as the covariance of jointly distributed random variables. These moment conditions are shown in the following equations and described in more detail below.

\begin{alignat*}{3}
	\text{Mean:} ~~ & \mu_y = E[y] ~ && \Rightarrow ~ & &E[y - \mu_y] = 0 \\
	\text{Variance:} ~~ & \sigma_y^2 = E[(y - \mu_y)^2] ~ && \Rightarrow ~ && E[ (y - \mu_y)^2 - \sigma_y^2] = 0 \\
	\text{Covariance:} ~~ & \sigma_{xy} = E[(y - \mu_y) (x - \mu_x)] ~ && \Rightarrow ~ && E[ (y - \mu_y) (x - \mu_x) - \sigma_{xy}] = 0
\end{alignat*}

\paragraph{Mean} Let $\mu_y$ be the mean of i.i.d.\ random variable $y$. Then, by definition, $\mu_y$ is the expectation of $y$, or $\mu_y = E[y]$. Rearranging this expression gives us a moment condition, $E[y - \mu_y] = 0$. This expression is a moment condition because it is a function of parameters and data, $y - \mu_y$, that equals zero in expectation.

\paragraph{Variance} Let $\sigma_y^2$ be the variance of i.i.d.\ random variable $y$. Then, by definition, $\sigma_y^2$ is the expectation of $(y - \mu_y)^2$, or $\sigma_y^2 = E[(y - \mu_y)^2]$. Rearranging this expression gives us a moment condition, $E[(y - \mu_y)^2 - \sigma_y^2] = 0$. This expression is a moment condition because it is a function of parameters and data, $(y - \mu_y)^2 - \sigma_y^2$, that equals zero in expectation.

\paragraph{Covariance} Let $\sigma_{xy}$ be the covariance of jointly distributed random variables $x$ and $y$. Then, by definition, $\sigma_{xy}$ is the expectation of $(y - \mu_y) (x - \mu_x)$, or $\sigma_{xy} = E[(y - \mu_y) (x - \mu_x)]$, where $\mu_x$ is the mean of $x$. Rearranging this expression gives us a moment condition, $E[(y - \mu_y) (x - \mu_x) - \sigma_{xy}] = 0$. This expression is a moment condition because it is a function of parameters and data, $(y - \mu_y) (x - \mu_x) - \sigma_{xy}$, that equals zero in expectation. \\

\noindent These example moment conditions are all generated by definitions of statistical objects, but we can generate moment conditions in many other ways. A common example is the first-order conditions of an economic model, which are functions of parameters and data that equal zero, meaning they can be used as moment conditions. Moment conditions can also come from econometric assumptions, such as the assumption that instruments must be orthogonal to model errors. A final example is a model fit criterion, such as ensuring that the predicted market shares of the model equal realized market shares in the data, which can be expressed as a moment condition. These are some common examples of moment conditions in structural econometric models, but it is far from an exhaustive list.

\subsection{Intuition of method of moments}

The basic intuition of method of moments estimation is best illustrated through an example. Suppose we have five random draws, $\bm{y} = \{5, 10, 9, 14, 7\}$, but we do not know the distribution from which these data were generated. Because we do not know the distribution, we cannot make the kind of distributional assumption required by maximum likelihood estimation. \\

\noindent Suppose we are interested in finding the mean of this unknown distribution, which we denote $\mu$. By definition, this mean gives us the expectation of random draw $y_i$, $E[y_i] = \mu$. We can rearrange this expression to yield a moment condition:
\begin{align*}
	E[y_i - \mu] & = 0
	\intertext{If this moment condition holds in expectation for the population, then we should expect an analogous condition to hold on average in a sample drawn from that population}
	\frac{1}{n} \sum_{i = 1}^n \left(y_i - \mu \right) & = 0
\end{align*}
We can estimate the population parameter by finding the parameter value that solves this empirical expression for our sample of data. \\

\noindent This simple example illustrates the basic intuition of method of moments estimation and the method of moments estimator---we know that certain moment conditions hold in expectation for the population, so we find the set of parameters that makes analogous moment conditions hold on average for our sample of data.

\section{Method of moments estimator}

Method of moments estimation requires moment conditions about the population from which our data are drawn. We assume that our data, $\{y, \bm{x}, \bm{z}\}$, are drawn from a population with $K$ moment conditions that are functions of $K$ parameters, $\bm{\theta}$, and these moment conditions are given by
$$E[\bm{m}(y, \bm{x}, \bm{z}, \bm{\theta})] = \bm{0}$$
where $\bm{m}(\cdot)$ is a vector of $K$ functions, $\bm{\theta}$ is a vector of $K$ parameters, $y$ is a dependent variable, $\bm{x}$ is a vector of independent variables, and $\bm{z}$ is a vector of exogenous instruments.\footnote{Independent variables that are exogenous may appear in both $\bm{x}$ and $\bm{z}$.} Note that we are making assumptions about the moments of this population but not its full density. \\

\noindent We seek to find the set of parameters, $\bm{\theta}$, that define these moment conditions and describe this population. We do not observe the full population, however, only a sample of data, so we cannot directly calculate the parameters. Instead, we replace the population expectation with its empirical analog---the sample mean---to generate the corresponding $K$ sample moment conditions as a function of the $K$ parameters:
$$\frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}) = \bm{0}$$ \\

\noindent The method of moments estimator, which we denote $\widehat{\bm{\theta}}$, is the set of parameters that solves the system of equations given by these sample moment conditions:
$$\frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widehat{\bm{\theta}}) = \bm{0}$$
In some cases, this system of equations will yield closed form expressions for the MM estimator. For more complex estimation, we can reformulate the MM estimator as the solution to a minimization problem and use numerical optimization. In this case, we define the MM estimator as
$$\widehat{\bm{\theta}} = \argmin_{\bm{\theta}} Q(\bm{\theta})$$
where the objective function, $Q(\bm{\theta})$, is given by
$$Q(\bm{\theta}) = \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}) \right]' \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}) \right]$$

\section{Examples of method of moments estimation}

\subsection{Population mean}

Suppose we have five random draws from a population:
$$\bm{y} = \{5, 10, 9, 14, 7\}$$
We do not know the distribution of this population but want to find its mean, which we denote $\mu$. By definition, if $y$ is i.i.d., then $\mu$ is equal to the expectation of $y$, or $\mu = E[y]$. Rearranging this expression gives a population moment condition:
\begin{align*}
	E[y - \mu] & = 0
	\intertext{We replace this population expectation with its empirical analog---the sample mean---yielding a sample moment condition. The MM estimator, $\widehat{\mu}$, is the value that solves this sample moment condition for our data:}
	\frac{1}{n} \sum_{i = 1}^n (y_i - \widehat{\mu}) & = 0
\end{align*}
We rearrange this sample moment condition to get an expression for the MM estimator, $\widehat{\mu}$:
\begin{align*}
	\widehat{\mu} & = \frac{1}{n} \sum_{i = 1}^n y_i
	\intertext{This expression defines the MM estimator, $\widehat{\mu}$, as a function of our data. In fact, the MM estimator for a population mean is simply the sample mean of the data. Plugging in our data yields the MM estimator:}
	\widehat{\mu} & = \frac{1}{n} \sum_{i = 1}^n y_i = 9
\end{align*}
In other words, we assume that the moment condition holds in expectation for the population, and a population mean of $\widehat{\mu} = 9$ makes the analogous moment condition hold on average for our sample of the data.

\subsection{Ordinary least squares regression}

In the previous example, we estimated the parameter that defined the moment condition for a single random variable. In most econometric applications, however, we model some outcome data, $y$, as a function of both parameters, $\bm{\theta}$, and other data, $\bm{x}$. An example is the general ordinary least squares (OLS) regression model:
$$y_i = \bm{\beta}' \bm{x}_i + \varepsilon_i$$
where $\bm{\beta}$ is a vector of $K$ parameters and $\bm{x}_i$ is a vector of $K$ variables.\footnote{One of these $K$ variables could be a constant or intercept term.} Note that OLS is a special case of MM estimation, so the MM estimator will be equivalent to the traditional OLS estimator, but we will find those parameter values in a different way. \\

\noindent In the OLS regression model, we assume the error term, $\varepsilon_i$, is orthogonal to the data, $\bm{x}_i$:
\begin{align*}
	E[\bm{x}_i \varepsilon_i] & = 0
	\intertext{Note that the error term, $\varepsilon_i$, can be expressed as the difference between the dependent variable and the fitted model value, $y_i - \bm{\beta}' \bm{x}_i$. Replacing $\varepsilon_i$ with $y_i - \bm{\beta}' \bm{x}_i$ gives $K$ population moment conditions:}
	E[\bm{x}_i (y_i - \bm{\beta}' \bm{x}_i)] & = \bm{0}
	\intertext{We replace the population expectation with its empirical analog---the sample mean---yielding $K$ sample moment conditions. The MM estimator, $\widehat{\beta}$, is the set of parameter values that solves these sample moment conditions for our data:}
	\frac{1}{n} \sum_{i = 1}^n \bm{x}_i (y_i - \widehat{\bm{\beta}}' \bm{x}_i) & = \bm{0}
\end{align*}
We rearrange these sample moment conditions to get an expression for the MM estimator, $\widehat{\beta}$, as a function of our data:
$$\widehat{\bm{\beta}} = \left(\sum_{i = 1}^n \bm{x}_i \bm{x}_i' \right)^{-1} \left(\sum_{i = 1}^n \bm{x}_i y_i \right)$$
This MM estimator for an OLS regression is equivalent to the traditional OLS estimator. Thus, we can interpret OLS parameters as not only minimizing the sum of squared error but also achieving orthogonality between the error terms and data.

\subsection{Maximum likelihood estimation}

One advantage of (generalized) method of moments estimation over maximum likelihood estimation is that we are not required to make an assumption about the full distribution of the population from which our data are drawn, only the moments of that population. If we do make a distributional assumption, however, then we can use MM estimation to find a MM estimator that is equivalent to the maximum likelihood estimator. \\

\noindent We assume that a random variable has conditional density $f(y_i \mid \bm{x}_i, \bm{\theta})$. Then the log-likelihood function of $\bm{\theta}$ conditional on $\bm{y}$ and $\bm{X}$ is
\begin{align*}
	\ln L(\bm{\theta} \mid \bm{y}, \bm{X}) & = \sum_{i = 1}^n \ln f(y_i \mid \bm{x}_i, \bm{\theta})
	\intertext{When we find the maximum likleihood estimator, we maximize this log-likelihood function, which yields $K$ first-order conditions, one for each of the $K$ parameters:}
	\bm{0} = \frac{\partial \ln L(\bm{\theta} \mid \bm{y}, \bm{X})}{\partial \bm{\theta}} & = \sum_{i = 1}^n \frac{\partial \ln f(y_i \mid \bm{x}_i, \bm{\theta})}{\partial \bm{\theta}}
\end{align*}
If we divide this expression by $n$, we get what look like sample moment conditions:
\begin{align*}
	\frac{1}{n} \sum_{i = 1}^n \frac{\partial \ln f(y_i \mid \bm{x}_i, \bm{\theta})}{\partial \bm{\theta}} & = \bm{0}
	\intertext{These $K$ sample moment conditions can be viewed as the empirical analogs of $K$ population moment conditions:}
	E \left[ \frac{\partial \ln f(y \mid \bm{x}, \bm{\theta})}{\partial \bm{\theta}} \right] & = \bm{0}
\end{align*}
If we used MM estimation with these population moment conditions, the MM estimator would satisfy the first-order conditions of the maximum likelihood estimator, so these two estimators would be equivalent. Thus, maximum likelihood estimation can be motivated as a special case of MM estimation with the population moment conditions given above.

\section{Generalized method of moments estimator}

The method of moments estimator requires that we have the same number of moment conditions and parameters to be estimated. In many empirical settings, however, we have more moment conditions than parameters to be estimated.\footnote{Additional moment conditions can come from many sources. A common example is estimating a model with more exogenous instruments than independent variables. Another example is that one economic model parameter might appear in multiple first-order conditions, which could generate more moments than parameters. These are only two of many possible sources of additional moment conditions.} When this occurs, we cannot use MM estimation and must instead use its more general form, generalized method of moments. \\

\noindent Generalized method of moments estimation requires moment conditions about the population from which our data are drawn. We assume that our data, $\{y, \bm{x}, \bm{z}\}$, are drawn from a population with $L$ moment conditions that are functions of $K$ parameters, $\bm{\theta}$, with $L \geq K$.\footnote{If we have fewer moments than parameters---that is, $L < K$---then the parameters of the model are not identified. If we have the same number of moments and parameters---that is, $L = K$---then we could use either MM estimation or GMM estimation to obtain the same estimator.} These moment conditions are given by
$$E[\bm{m}(y, \bm{x}, \bm{z}, \bm{\theta})] = \bm{0}$$
where $\bm{m}(\cdot)$ is a vector of $L$ functions, $\bm{\theta}$ is a vector of $K$ parameters, $y$ is a dependent variable, $\bm{x}$ is a vector of independent variables, and $\bm{z}$ is a vector of exogenous instruments.\footnote{Independent variables that are exogenous may appear in both $\bm{x}$ and $\bm{z}$.} Note that we are making assumptions about the moments of this population but not its full density. \\

\noindent We seek to find the set of parameters, $\bm{\theta}$, that define these moment conditions and describe this population. We do not observe the full population, however, only a sample of data, so we cannot directly calculate the parameters. Instead, we replace the population expectation with its empirical analog---the sample mean---to generate the corresponding $L$ sample moment conditions as a function of the $K$ parameters:
$$\frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}) = \bm{0}$$ \\
We cannot solve for $L$ unique sample moment conditions with only $K$ parameters when $L > K$; we describe this model as being overidentified. Instead, we will minimize the weighted sum of the sample moments, effectively getting as close as possible to solving all $L$ sample moment conditions. \\

\noindent The generalized method of moments estimator, which we denote $\widehat{\bm{\theta}}$, is the set of parameters that minimizes the weighted sum of the sample moments:
$$\widehat{\bm{\theta}} = \argmin_{\bm{\theta}} Q(\bm{\theta})$$
where the objective function, $Q(\bm{\theta})$, is given by
$$Q(\bm{\theta}) = \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}) \right]' \bm{W} \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}) \right]$$
and $\bm{W}$ is a $L \times L$ positive definite weighting matrix. We will discuss the choice of weighting matrix after looking at an example and describing the properties of the GMM estimator.

\section{Example of generalized method of moments estimation}

A common example of an econometric model that could be overidentified is an instrumental variables linear regression model:
$$y_i = \bm{\beta}' \bm{x}_i + \varepsilon_i$$
where $\bm{\beta}$ is a vector of $K$ parameters and $\bm{x}_i$ is a vector of $K$ variables,\footnote{One of these $K$ variables could be a constant or intercept term.} some of which are endogenous. Because of the endogeneity of $\bm{x}_i$, estimating this model by OLS would yield biased and inconsistent estimates of the true causal parameters, $\bm{\beta}$. Instead, we instrument for the endogenous variables with a set of $L$ exogenous instruments, which we denote $\bm{z}_i$.\footnote{Independent variables that are exogenous may appear in both $\bm{x}_i$ and $\bm{z}_i$.} We must have at least as many exogenous instruments as independent variables, so $L \geq K$. \\

\noindent Valid instruments must be orthogonal to the error term from the above regression model, $\varepsilon_i$:
\begin{align*}
	E[\bm{z}_i \varepsilon_i] & = 0
	\intertext{Note that the error term, $\varepsilon_i$, can be expressed as the difference between the dependent variable and the fitted model value, $y_i - \bm{\beta}' \bm{x}_i$. Replacing $\varepsilon_i$ with $y_i - \bm{\beta}' \bm{x}_i$ gives the population moment conditions:}
	E[\bm{z}_i (y_i - \bm{\beta}' \bm{x}_i)] & = \bm{0}
	\intertext{We have $L$ instruments and, hence, $L$ population moment conditions that are functions of $K$ parameters, with $L \geq K$. Thus, this model could be overidentified, so we will use GMM estimation. If we replace the population expectation with its empirical analog---the sample mean---we obtain $L$ sample moment conditions that are functions of $K$ parameters.}
	\frac{1}{n} \sum_{i = 1}^n \bm{z}_i (y_i - \bm{\beta}' \bm{x}_i) & = \bm{0}
\end{align*}
We cannot solve a system of $L$ unique equations with $K$ parameters if $L > K$. Instead, we find the GMM estimator, $\widehat{\bm{\beta}}$, by minimizing the weighted sum of the sample moments:
$$\widehat{\bm{\beta}} = \argmin_{\bm{\beta}} Q(\bm{\beta})$$
where the objective function, $Q(\bm{\beta})$, is given by
$$Q(\bm{\beta}) = \left[ \frac{1}{n} \sum_{i = 1}^n \bm{z}_i (y_i - \bm{\beta}' \bm{x}_i) \right]' \bm{W} \left[ \frac{1}{n} \sum_{i = 1}^n \bm{z}_i (y_i - \bm{\beta}' \bm{x}_i) \right]$$
Note that this objective function depends on the choice of weighting matrix, $\bm{W}$, so the GMM estimator also depends on the weighting matrix. If we use a weighting matrix of
$$\bm{W} = \sum_{i = 1}^n \bm{z}_i \bm{z}_i'$$
then the GMM estimator, $\widehat{\bm{\beta}}$, is equivalent to the 2SLS estimator, indicating that the 2SLS parameters achieve orthogonality between the error terms and exogenous instruments. If we use a different weighting matrix, however, then we will obtain a different GMM estimator. \\

\noindent The dependence of the GMM estimator on the weighting matrix, $\bm{W}$, highlights the importance of the choice of weighting matrix. In order to describe the optimal weighting matrix, we first need to know the properties of the GMM estimator.

\section{Properties of the generalized method of moments estimator}

The GMM estimator is consistent and asymptotically normal when the assumed population moment conditions are valid and and the empirical moments and weighting matrix meet certain conditions. Note that, unlike the maximum likelihood estimator, the GMM estimator is not efficient.\footnote{The distributional assumption of maximum likelihood estimation provides additional information that leads to its efficiency.} \\

\noindent The required conditions on the empirical moments and weighting matrix are shown below. In this and future sections, $\widehat{\bm{\theta}}$ denotes the GMM estimator, $\bm{\theta}_0$ denotes the true values of the parameters, $\bm{\theta}$ denotes any arbitrary set of parameters, and the $0$ subscript denotes an object based on the true parameters values. 
\begin{enumerate}
	\item The empirical moments obey the law of large numbers:
	$$\frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}_0) \overset{p}{\rightarrow} \bm{0}$$
	\item The derivatives of the empirical moments converge:
	$$\frac{1}{n} \sum_{i = 1}^n \left. \frac{\partial \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta})}{\partial \bm{\theta}'} \right\vert_{\bm{\theta} = \bm{\theta}_0} \overset{p}{\rightarrow} \bm{G}_0$$
	\item The empirical moments obey the central limit theorem:
	$$\frac{\sqrt{n}}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}_0) \overset{d}{\rightarrow} \mathcal{N}(0, \bm{S}_0)$$
	where
	$$\bm{S}_0 = \plim \frac{1}{n} \sum_{i = 1}^n \sum_{j = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}_0) \bm{m}(y_j, \bm{x}_j, \bm{z}_j, \bm{\theta}_0)'$$
	\item The parameters are identified:
	$$\frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}_1) = \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}_2) ~~\Leftrightarrow~~ \bm{\theta}_1 = \bm{\theta}_2$$
	\item The weighting matrix converges to a finite symmetric positive definite matrix:
	$$\bm{W} \overset{p}{\rightarrow} \bm{W}_0$$
\end{enumerate}
These conditions ensure that the empirical moments and weighting matrix meet some basic requirements for being ``well-behaved.'' When these conditions are met and the population moments are valid, the GMM estimator has the properties listed above, which are described in more detail below.\footnote{For the proofs of these properties, see \fullcite{greeneEconometricAnalysis2018}.}

\paragraph{Consistency} The GMM estimator converges in probability to the true parameters values:
$$\widehat{\bm{\theta}} \overset{p}{\rightarrow} \bm{\theta}_0$$
In words, as the sample size increases to infinity, the GMM estimator becomes vanishingly close to the true parameter values.

\paragraph{Asymptotic normality} The GMM estimator is asymptotically normal with a mean at the true parameter values and known variance:
$$\widehat{\bm{\theta}} \overset{a}{\sim} \mathcal{N}\left( \bm{\theta}_0, \frac{1}{n} (\bm{G}_0' \bm{W}_0 \bm{G}_0)^{-1} (\bm{G}_0' \bm{W}_0 \bm{S}_0 \bm{W}_0 \bm{G}_0) (\bm{G}_0' \bm{W}_0 \bm{G}_0)^{-1} \right)$$
where
\begin{align*}
	\bm{G}_0 & = \plim \frac{1}{n} \sum_{i = 1}^n \left. \frac{\partial \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta})}{\partial \bm{\theta}'} \right\vert_{\bm{\theta} = \bm{\theta}_0} \\
	\bm{W}_0 & = \plim \bm{W} \\
	\bm{S}_0 & = \plim \frac{1}{n} \sum_{i = 1}^n \sum_{j = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}_0) \bm{m}(y_j, \bm{x}_j, \bm{z}_j, \bm{\theta}_0)'
\end{align*}
In words, the asymptotic variance of the GMM estimator depends on the probability limit of the gradients of the empirical moments evaluated at the true parameter values, the probability limit of the weighting matrix, and the probability limit of the variance-covariance matrix of the empirical moments evaluated at the true parameter values.

\section{Optimal generalized method of moments estimator}

From these properties of the GMM estimator, we see that the consistency of the GMM estimator does not depend on the choice of weighting matrix, $\bm{W}$, but the asymptotic variance of the GMM estimator does. Thus, every weighting matrix yields a GMM estimator that is consistent, but the choice of weighting matrix will affect the efficiency of the estimator. We would ideally like to use the weighting matrix that minimizes the variance of the GMM estimator. \\

\noindent The asymptotic variance of the GMM estimator is minimized when the probability limit of the weighting matrix, $\bm{W}_0$, equals the inverse of $\bm{S}_0$, which was defined above:
$$\bm{W}_0 = \bm{S}_0^{-1} = \left[ \plim \frac{1}{n} \sum_{i = 1}^n \sum_{j = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}_0) \bm{m}(y_j, \bm{x}_j, \bm{z}_j, \bm{\theta}_0)' \right]^{-1}$$
This weighting matrix causes many terms to cancel in the asymptotic variance, resulting in a simpler expression for the asymptotic variance:
$$Var(\widehat{\bm{\theta}}) = \frac{1}{n} (\bm{G}_0' \bm{S}_0^{-1} \bm{G}_0)^{-1}$$
Note that $\bm{S}_0$ is the probability limit of the variance-covariance matrix of the empirical moments evaluated at the true parameter values, so we minimize the the asymptotic variance of the GMM estimator by effectively weighting each moment inversely to its variance. \\

\noindent This GMM estimator, which has the smallest variance of all GMM estimators, is called the optimal GMM estimator. To estimate it, we want to use a weighting matrix, $\bm{W}$, that converges in probability to $\bm{S}_0$, the probability limit of the variance-covariance matrix of the empirical moments evaluated at the true parameter values. We do not know the true parameter values, however, so we cannot directly evaluate this matrix. Instead, we can use a two-step procedure to first estimate $\bm{S}_0$ and then use this estimate to find the optimal GMM estimator, which is sometimes called the two-step GMM estimator.\footnote{This procedure can be viewed as a generalization of a feasible generalized least squares regression.} \\

\noindent In the first step, we find a GMM estimator using any arbitrary weighting matrix---a common choice is the identity matrix. We denote this first-step GMM estimator as $\widetilde{\bm{\theta}}$. Because the GMM estimator is consistent for any weighting matrix, $\widetilde{\bm{\theta}}$ is a consistent estimate of the true parameter values, $\bm{\theta}_0$. We then use this first-step GMM estimator, $\widetilde{\bm{\theta}}$, to construct an estimator of $\bm{S}_0$. If we assume observations are independent, this estimator is
$$\widetilde{\bm{S}} = \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widetilde{\bm{\theta}}) \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widetilde{\bm{\theta}})'$$ \\
In the second step, we find the optimal GMM estimator using a weighting matrix based on this estimate of $\bm{S}_0$, $\bm{W} = \widetilde{\bm{S}}^{-1}$. That is, we define the optimal GMM estimator as
$$\widehat{\bm{\theta}} = \argmin_{\bm{\theta}} Q(\bm{\theta})$$
where the objective function, $Q(\bm{\theta})$, is given by
$$Q(\bm{\theta}) = \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}) \right]' \widetilde{\bm{S}}^{-1} \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta}) \right]$$
The GMM estimator we obtain in this second step, $\widehat{\bm{\theta}}$, also gives a consistent estimate of the true parameter values, $\bm{\theta}_0$, and it has the minimum variance among all GMM estimators. Note that the optimal GMM estimator is not efficient among all estimators---for example, when compared to the maximum likelihood estimator---but it is efficient among GMM estimators. \\

\noindent The optimal GMM estimator has an asymptotic variance of
$$Var(\widehat{\bm{\theta}}) = \frac{1}{n} (\bm{G}_0' \bm{S}_0^{-1} \bm{G}_0)^{-1}$$
To calculate this variance-covariance matrix, we must evaluate the probability limit of the gradients of the empirical moments and the probability limit of the variance-covariance matrix of the empirical moments evaluated at the true parameter values. We do not know the true parameter values, however, so we cannot use this expression directly. Instead, an estimator for the variance-covariance matrix is
$$\widehat{Var}(\widehat{\bm{\theta}}) = \frac{1}{n} \left( \widehat{\bm{G}}' \widehat{\bm{S}}^{-1} \widehat{\bm{G}} \right)^{-1}$$
where the components of this matrix are estimated by
\begin{align*}
	\widehat{\bm{G}} & = \frac{1}{n} \sum_{i = 1}^n \left. \frac{\partial \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \bm{\theta})}{\partial \bm{\theta}'} \right\vert_{\bm{\theta} = \widehat{\bm{\theta}}} \\
	\widehat{\bm{S}} & = \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widehat{\bm{\theta}}) \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widehat{\bm{\theta}})'
\end{align*}
That is, we estimate the variance-covariance matrix of the optimal GMM estimator by evaluating the gradients and variance-covariance matrix of the empirical moments at the optimal GMM estimator.\footnote{There are alternate variance estimators for the GMM estimator that are more robust, but this most basic estimator will be sufficient for this course.}

\section{Specification and hypothesis tests}

\subsection{Overidentifying restrictions test}

When we have more moment conditions than parameters, the model is overidentified and we cannot ensure all sample moment conditions hold. We can, however, perform an overidentifying restrictions test to determine if the model is misspecified and, hence, not all population moment conditions hold. The null hypothesis of the overidentifying restrictions test is
$$H_0 \text{: } E[\bm{m}(y, \bm{x}, \bm{z}, \bm{\theta}_0)] = \bm{0}$$
When $\widehat{\bm{\theta}}$ is the optimal GMM estimator, the overidentifying restrictions test statistic is equal to the GMM objective function evaluated at the optimal GMM estimator, $Q(\widehat{\bm{\theta}})$:
$$OIR = \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widehat{\bm{\theta}}) \right]' \widetilde{\bm{S}}^{-1} \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widehat{\bm{\theta}}) \right]$$
This test statistic is asymptotically distributed $\chi^2$ with $L - K$ degrees of freedom
$$OIR \overset{a}{\sim} \chi^2(L - K)$$
If the test statistic is sufficiently large, then we reject that the population moment conditions are valid, concluding that the model is misspecified and the GMM estimator is inconsistent for the true parameter values, $\bm{\theta}_0$.

\subsection{Hypothesis tests}

There are three common test procedures to test hypotheses about parameters that use the results of GMM estimation: difference test, Wald test, and Lagrange multiplier test.\footnote{The difference test for GMM is analogous to the likelihood ratio test for maximum likelihood estimation. The Wald and Lagrange multiplier tests for GMM are analogous to the same-named tests for maximum likelihood estimation.} Consider the test of hypotheses:
$$H_0 \text{: } \bm{h}(\bm{\theta}_0) = \bm{0}$$
where $\bm{h}(\bm{\theta}_0)$ is any set of $J$ parameter restrictions. This specification of hypotheses is fully general. For example, it can represent a test that parameters equal zero:
$$\bm{h}(\bm{\theta}_0) = \begin{pmatrix}
  \theta_1 \\
  \theta_2
\end{pmatrix} = \begin{pmatrix}
  0 \\
  0
\end{pmatrix}$$
or a test that parameters equal one another:
$$\bm{h}(\bm{\theta}_0) = \begin{pmatrix}
  \theta_1 - \theta_3 \\
  \theta_2 - \theta_3
\end{pmatrix} = \begin{pmatrix}
  0 \\
  0
\end{pmatrix}$$
or any other hypothesis test about the parameters. \\

\noindent The following test statistics apply to the optimal GMM estimator of the unrestricted model and the GMM estimator of the restricted model obtained using the unrestricted optimal weighting matrix. More complex test statistics apply to other GMM estimators.

\paragraph{Difference test} If the hypotheses are true, then the objective function value, $Q(\bm{\theta})$, should be approximately the same whether the hypothesized restrictions are imposed or not. That is, $Q(\widehat{\bm{\theta}}_R) \approx Q(\widehat{\bm{\theta}}_U)$ where $\widehat{\bm{\theta}}_R$ is the GMM estimator of the restricted model that is obtained with the hypothesized restrictions imposed and $\widehat{\bm{\theta}}_U$ is the GMM estimator of the unrestricted model that is obtained without those restrictions. The difference test draws on this intuition and tests if $Q(\widehat{\bm{\theta}}_R)$ is sufficiently close to $Q(\widehat{\bm{\theta}}_U)$. The difference test statistic is
$$D = n \left( Q(\widehat{\bm{\theta}}_R) - Q(\widehat{\bm{\theta}}_U) \right)$$
The difference test statistic has an asymptotic chi-squared distribution with degrees of freedom equal to the number of restrictions, $J$:
$$D \overset{a}{\sim} \chi^2(J)$$
Note that the difference test requires estimating two models---the restricted model that is obtained with the hypothesized restrictions imposed and the unrestricted model that is obtained without these restrictions---and calculating the objective function value of each model. Once these objective function values are obtained, the test statistic is simple to calculate.

\paragraph{Wald test} If the hypotheses are true, then the same functional transformations applied to the GMM estimator should be close to zero. That is, $\bm{h}(\widehat{\bm{\theta}}) \approx \bm{0}$. The Wald test draws on this intuition and tests if $\bm{h}(\widehat{\bm{\theta}})$ is sufficiently close to $\bm{0}$. The Wald test statistic is
$$W = n \bm{h}(\widehat{\bm{\theta}})' \left[ \widehat{\bm{H}} \left( \widehat{\bm{G}}' \widehat{\bm{S}}^{-1} \widehat{\bm{G}} \right)^{-1} \widehat{\bm{H}}' \right]^{-1} \bm{h}(\widehat{\bm{\theta}})$$
where $\widehat{\bm{G}}$ and $\widehat{\bm{S}}$ are as defined above and $\widehat{\bm{H}}$ is the $J \times K$ matrix of derivatives of $\bm{h}(\bm{\theta})$ with respect to $\bm{\theta}$ evaluated at the GMM estimator:
$$\widehat{\bm{H}} = \left. \frac{\partial \bm{h}(\bm{\theta})}{\partial \bm{\theta}'} \right\vert_{\bm{\theta} = \widehat{\bm{\theta}}}$$
The Wald test statistic has an asymptotic chi-squared distribution with degrees of freedom equal to the number of restrictions, $J$:
$$W \overset{a}{\sim} \chi^2(J)$$
Note that the Wald test requires estimating only the unrestricted model, but the additional calculations of the test statistic are more involved than those of the difference test.

\paragraph{Lagrange multiplier test} If the hypotheses are true, then the GMM estimator of the restricted model should be close to the GMM estimator of the unrestricted model. The slope of the objective function at the unrestricted GMM estimator is zero, so the slope of the objective function at the restricted GMM estimator should be close to zero. That is, $\partial Q(\bm{\theta}) / \partial \bm{\theta} \approx \bm{0}$ when evaluated at the restricted GMM estimator, $\widehat{\bm{\theta}}_R$. The Lagrange multiplier test draws on this intuition and tests if $\partial Q(\bm{\theta}) / \partial \bm{\theta}$ evaluated at $\widehat{\bm{\theta}}_R$ is sufficiently close to $\bm{0}$. The Lagrange multiplier test statistic is
$$LM = n \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widehat{\bm{\theta}}_R) \right]' \widehat{\bm{S}}^{-1} \widehat{\bm{G}}_R \left( \widehat{\bm{G}}_R' \widehat{\bm{S}}^{-1} \widehat{\bm{G}}_R \right)^{-1} \widehat{\bm{G}}_R' \widehat{\bm{S}}^{-1} \left[ \frac{1}{n} \sum_{i = 1}^n \bm{m}(y_i, \bm{x}_i, \bm{z}_i, \widehat{\bm{\theta}}_R) \right]$$
where $\widehat{\bm{S}}$ is as defined above and $\widehat{\bm{G}}_R$ is similar to above but evaluated at the restricted GMM estimator, $\widehat{\bm{\theta}}_R$.
The Lagrange multiplier test statistic has an asymptotic chi-squared distribution with degrees of freedom equal to the number of restrictions, $J$:
$$LM \overset{a}{\sim} \chi^2(J)$$
Note that the Lagrange multiplier test requires estimating only the restricted model, but the additional calculations of the test statistic are more involved than those of the difference test.

\end{document}